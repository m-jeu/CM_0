{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Research iteration 1</h1>\n",
    "\n",
    "<i>Sjoerd Beetsma, Maarten de Jeu\n",
    "Class V2A - Group 5</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "\n",
    "TODO(m-jeu): Place to write a quick introduction to the notebook structure, conventions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Business Understanding</h2>\n",
    "\n",
    "Through the business, we have been asked to examine the following questions:\n",
    "\n",
    "<ol>\n",
    "<li>In hoeverre is de score van een Portugese Red te voorspellen op basis van de chemische kenmerken?</li>\n",
    "<li>Zelf te bepalen: denk aan andere type wijnen of andere landen.</li>\n",
    "<li>Zelf te bepalen: denk bijvoorbeeld aan het clusteren op basis van de chemische kenmerken waarmee het type druif of de regio bepaald kan worden. Of kun je logische clusters vinden van topwijnen, doordrinkwijnen en bocht?</li>\n",
    "</ol>\n",
    "\n",
    "<i>TODO(m-jeu): Either translate these questions to english, or change the language of the rest of the document to dutch. This also goes for the dutch list in 'Data Understanding'.</i>\n",
    "\n",
    "Currently, the exact nature of 'the business' and their desires datascience-wise is unknown to us. This requires further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Understanding </h2>\n",
    "\n",
    "The business tells us the most important variables in the dataset are:\n",
    "\n",
    "<ol>\n",
    "<li>Herkomst van de wijn en type druif.</li>\n",
    "<li>Review van de wijn, inclusief naam van de sommelier en de score op een schaal van 1 tot 100.</li>\n",
    "<li>De uitkomsten van chemische tests op 11 waarde (waaronder suikergehalte, pH, alcoholgehalte, et cetera).</li>\n",
    "\n",
    "We import some libraries, and the dataset. Then we have an initial look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/redwine.csv\", sep=\";\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows seems to correspond with individual wines on first glance, though this does need to be examined more thoroughly. Columns are different attributes for those individual wines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature variables</h3>\n",
    "\n",
    "Here we remove the columns: country, variety, description, title, taster_name and id. They are unnecessary for our model in determining the Target, which is points.\n",
    "Country and variety are possible feature variables but are left out for now as the dataset contains only 1 unique country and variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_features_target = dataset.drop(['country', 'variety', 'description', 'title', 'taster_name', 'id'], axis=1)\n",
    "dataset_features_target.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Scales of measurements </h3>\n",
    "\n",
    "To construct an appropiate model it's necessary to have a understanding of all the scales of measurements for the target and feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nom, disc, cont = 'Nominal', 'Discrete', 'Continous'\n",
    "vars_scale = [nom, disc, disc, nom, nom, cont, cont, cont, cont, cont, cont, cont, cont, cont, cont, cont ]\n",
    "measurement_scales = pd.DataFrame({'Variable':dataset_features_target.columns, 'Scale of measurement':vars_scale})\n",
    "measurement_scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Central tendancies and dispersion measures</h3>\n",
    "\n",
    "From the central tendancies and dispersion measures we can see some useful statistics about the target and feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_features_target.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be noted the feature variables alcohol, density, citric acid aren't described even though they are of discrete scale, we will look at what's wrong during the data preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Distribution of data</h3>\n",
    "\n",
    "Lets take a more visual look at the distribution of all the variables through a histogram for each of the feature and target attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features_target.hist(figsize=(15,15), bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of pH seems to follow a normal distribution.\n",
    "From the points and price histogram we can see that most wines range from a relatively high score between 85 and 90. And most wines ranging in a price from around 5 to 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Outliers</h3>\n",
    "\n",
    "To get a visual understanding of the outliers in the feature columns each feature gets a boxplotted with the target variable points. Giving a small summary of the minimum, Q1, Q2 (median), Q3 and the maximum of each attribute plotted against points scored to give a view of outliers at all quality/point levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in dataset_features_target.select_dtypes(include=np.number, exclude=None).drop('points', axis=1): # x-axis requires numerical value and don't plot id\n",
    "    sns.boxplot(x=dataset['points'], y=dataset[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the boxplots all of our current variables (beside the ones that need to be cleaned before further data understanding) contain outliers.\n",
    "\n",
    "All outliers in the above boxplots seem to be plausible and not from incorrect data.\n",
    "From the boxplot with price on the y axis and points on the x axis we can clearly see that the price of a wine is related to the quality of the wine.\n",
    "Also we can tell that wines with a quality of 92 or higher have a volatile acidity below 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Correlations</h3>\n",
    "\n",
    "To help find positive, negative and neutral correlations matrix is constructed where dark red corresponds to a positive correlation and green a negative correlation.\n",
    "\n",
    "We create this correlation matrix with the help of a function, so that we can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_matrix(dataset: pd.DataFrame):\n",
    "    \"\"\"Return a correlation matrix created using seaborn and matplotlib that for all columns in\n",
    "    a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset to construct correlation matrix for.\n",
    "\n",
    "    Returns:\n",
    "        correlation matrix.\"\"\"\n",
    "    corr = dataset.corr()\n",
    "    plt.figure(figsize=(10,7.5))\n",
    "    cmap = sns.diverging_palette(200, 0, as_cmap=True) # color palette as cmap\n",
    "    mask = np.logical_not(np.tril(np.ones_like(corr))) # triangle mask\n",
    "    sns.heatmap(corr,annot=True, mask=mask, cmap = cmap, vmin=-1, vmax=1) # correlation heatmap\n",
    "\n",
    "\n",
    "correlation_matrix(dataset_features_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the correlation matrix graph above you can see which attributes have a correlation to other attributes. Starting with our target variable 'quality', we can see quality has a few correlations with the strongest one being alcohol and a few weaker ones like volatile acidity, sulphates and citric acid. Because quality is our target variable it's independent attribute in the correlation.\n",
    "\n",
    "Getting rid of outliers might bring the absolute value of the correlation scores closer to one, but this would be something to look at again during data preparation.\n",
    "\n",
    "Besides there are some correlations among chemical properties:\n",
    "Fixed acidity has strong correlation with pH, but itâ€™s still an independent type. pH However is a dependent type; it depends on the former. Volatile acidity, residual sugar, sulphates, chlorides, and density are all independent data types. Total sulfur dioxide is dependent on free sulfur dioxide, but free sulfur dioxide is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preparation</h2>\n",
    "\n",
    "The data needs some cleaning up. Because it might be useful to be able to view columns that were droppped in Data Exploration to gain more understanding, like wine titles, we'll be operating on the original dataset.\n",
    "\n",
    "The current policy we'll be adhering to outlier-wise, is that outliers that are somewhat realistic won't be dropped (for now). Our philosophy is that these might still be relevant for some models, so we won't drop them until absolutely necessary. Outliers that are obvious errors will be dropped.\n",
    "\n",
    "An overview of datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter doesn't recognize some of the Python str objects for what they are, and simply calls them the 'object' type. Let's convert them to the right type to allow for more method flexibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.convert_dtypes()\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some datatypes in the DataFrame that don't quite correspond to what you'd expect them to be, considering what they represent. Citric acid, Density and Alcohol are string objects, even though you'd expect them to be some kind of number-datatype. Let's copy everyone to a seperate column called 'raw_columnname' so that we can evaluate the original next to the converted in case we run into any problems turning these columns into numbers. Then we'll convert each one individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problematic_column_names = [\"citric acid\", \"density\", \"alcohol\"]\n",
    "copied_column_names = {f\"raw_{name}\":name for name in problematic_column_names}\n",
    "\n",
    "for new, to_copy in copied_column_names.items():  # Is this allowed?\n",
    "    dataset[new] = dataset[to_copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preparation on column-by-column basis</h3>\n",
    "\n",
    "<h4>Citric Acid</h4>\n",
    "\n",
    "The citric acid column consists of string objects, through most entries are formatted like floats. Pandas can convert these for us, turning the ones that it can't understand into not-a-number entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"citric acid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"citric acid\"] = pd.to_numeric(dataset[\"citric acid\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's define a simple convenience function that describes the min, max and amount of nan entries in a series object, to quickly gauge the validity of data contained within, and call it on the Citric Acid column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def simple_describe(s: pd.Series, name: str = \"Series object\") -> None:\n",
    "    \"\"\"Print out a simple description of a Pandas Series object that contains numeric values.\n",
    "    That covers min, max and amount of nan/None entries.\n",
    "\n",
    "    Args:\n",
    "        s: Pandas series object with numeric values.\n",
    "        name: optional name for the series to use in the printed description.\"\"\"\n",
    "    print(f\"{name}:\\nmin: {np.min(s)}\\nmax: {np.max(s)}\\n#(nan): {s.isnull().sum()}\")\n",
    "\n",
    "simple_describe(dataset[\"citric acid\"], \"Citric acid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto converting the citric acid table to float leaves us with 203 not-a-number entries. Lets have a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.loc[dataset[\"citric acid\"].isnull()][[\"id\", \"title\", \"citric acid\", \"raw_citric acid\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears these data points did not have a value for citric acid to begin with. According to <a href=\"https://wineserver.ucdavis.edu/industry-info/enology/methods-and-techniques/common-chemical-reagents/citric-acid\">this website</a>, citric acid is something that's <i>added</i> to wine most of the time, but it doesn't seem essential. We'll assume that the wines with '-' for citric acid were given this value on purpose, and that this means that there's none in there. Let's replace those values with 0 in the raw table, and use that to create a final version of the citric acid table. Then we'll describe it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset['raw_citric acid'] = dataset['raw_citric acid'].str.replace('-', '0')\n",
    "dataset['citric acid'] = pd.to_numeric(dataset[\"raw_citric acid\"], errors='coerce')\n",
    "simple_describe(dataset['citric acid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citric acid seems clean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Density</h4>\n",
    "\n",
    "Most density values are formatted like proper floats, so let's have pandas turn them into that, and then describe it. This time, the simple describe won't quite cut it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"density\"] = pd.to_numeric(dataset[\"density\"], errors='coerce')\n",
    "dataset[\"density\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the description, we see that almost all values (the mininum, and all quartiles) fall around the 0.99 range. Let's examine the spread of data further with the help of a frequency graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(dataset[\"density\"], bins=50)  # TODO(m-jeu): Use OOP API and make clearer\n",
    "plt.gca().set(title=\"Density frequency\", xlabel=\"Density\", ylabel=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most measurements fall around the '0.99' mark. Though some fall around the 10, and a couple fall around the 100 mark. We will assume that this is a notation error, where the dots weren't placed right, which can fairly easily be accounted for. If we assume that all measurements are supposed to be around '1', because it wouldn't be realistic for wines to have a 10x or 100x difference in density with others, we can correct for the mistake by recursively dividing the number by 10, until a more sensible measurement is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def divide_until_at(num: float,\n",
    "                    target: float,\n",
    "                    divide_by: float,\n",
    "                    delta: float,\n",
    "                    max_recursions: int = 0) -> float:\n",
    "    \"\"\"Recursively divide a number until it's either within a certain range of a target number,\n",
    "    or the max number of recursions is reached.\n",
    "\n",
    "    Args:\n",
    "        num: input number to divide.\n",
    "        target: target number to get within a certain range of.\n",
    "        divide_by: number to divide input number by.\n",
    "        delta: amount of distance the input number could be from the target number to be considered a success.\n",
    "        max_recursion: amount of recursion after which the function should give up.\n",
    "\n",
    "    Returns:\n",
    "        num divided by divide_by a certain amount of times, so that it's within delta of target.\n",
    "        amount of times num is divided can also be 0.\n",
    "        None if max_recursions gets violated.\"\"\"\n",
    "    if max_recursions < 0:\n",
    "        return None\n",
    "    if abs(num - target) < delta:\n",
    "        return num\n",
    "    return divide_until_at((num / divide_by), target, divide_by, delta, (max_recursions - 1))\n",
    "\n",
    "dataset[\"density\"] = dataset[\"density\"].apply(lambda x: divide_until_at(x, 1, 10, 0.1, 2))\n",
    "dataset[\"density\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simple_describe(dataset[\"density\"], \"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weird numbers have been taken care of. A single not-a-number entry remains. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.loc[dataset[\"density\"].isnull()][['id', 'title', 'density', 'raw_density']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single wine has a density of '.', which we can't really use for anything. To keep the data pure, let's drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[\"density\"].notna()]\n",
    "simple_describe(dataset[\"density\"], \"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density is clean!\n",
    "\n",
    "<h4>Alcohol</h4>\n",
    "\n",
    "Most alcohol percentages are formatted like proper floats, and can be converted automatically. This allows us to analyse the actual numbers as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"alcohol\"] = pd.to_numeric(dataset[\"alcohol\"], errors='coerce')  # No OOP interface.\n",
    "simple_describe(dataset[\"alcohol\"], \"Alcohol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The alcohol-percentages we have been able to extract from the String objects seem to fall within the boundaries of what could reasonably be considered 'wine'. We are still left with 10 not-a-numbers entries where pandas wasn't able to convert the strings to numbers. Let's have a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.loc[dataset[\"alcohol\"].isnull()][['id', 'title', 'designation', 'alcohol']]  # TODO(m-jeu): This can probably be made prettier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering only 10 wines are missing data, we can probably look these up by hand. The percentages are based on these sources:\n",
    "\n",
    "<a href=\"https://www.internationalwinechallenge.com/canopy/beverage_details?wid=44074\">1344. </a><a href=\"https://www.winemag.com/buying-guide/companhia-das-quintas-2009-quinta-da-fronteira-seleccao-do-enologo-red-douro/\">1345. </a><a href=\"https://www.winemag.com/buying-guide/quinta-das-bandeiras-2010-passagem-reserva-red-douro/\">1620. </a><a href=\"https://www.nataliemaclean.com/wine-reviews/quinta-da-rede-reserva-2015/317025\">1670. </a><a href=\"https://www.winemag.com/buying-guide/terra-silvestre-2014-grande-reserva-red-tejo-portuguese-red/\">1764. </a><a href=\"https://www.vivino.com/NL/en/adega-cooperativa-de-borba-alentejo-reserva-tinto/w/1235048?year=2013\">1765. </a><a href=\"https://www.winemag.com/buying-guide/adega-cooperativa-cartaxo-2012-bridao-classico-red-tejo/\">1766. </a><a href=\"https://www.winemag.com/buying-guide/quinta-da-lagoalva-de-cima-2015-lagoalva-barrel-selection-red-tejo/\">1794. </a><a href=\"https://www.winemag.com/buying-guide/quinta-do-casal-branco-2014-lobo-e-falcao-reserva-red-tejo/\">1795. </a><a href=\"https://www.wine-searcher.com/find/ramos+pinto+duas+quinta+rsrv+douro+portugal/2006/netherlands#t2\">2028. </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Assignments use numeric row index, and not 'id' column. Might break when working on the id column.\n",
    "dataset.loc[1344, \"alcohol\"] = 14.0\n",
    "dataset.loc[1345, \"alcohol\"] = 14.5\n",
    "dataset.loc[1620, \"alcohol\"] = 14.0\n",
    "dataset.loc[1670, \"alcohol\"] = 13.5\n",
    "dataset.loc[1764, \"alcohol\"] = 13.5\n",
    "dataset.loc[1765, \"alcohol\"] = 14.0\n",
    "dataset.loc[1766, \"alcohol\"] = 14.0\n",
    "dataset.loc[1794, \"alcohol\"] = 14.0\n",
    "dataset.loc[1795, \"alcohol\"] = 14.0\n",
    "dataset.loc[2028, \"alcohol\"] = 14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' have a look how many nan entries are left in the alcohol column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"alcohol\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcohol is clean!\n",
    "\n",
    "Having converted and cleaned up all tables in the dataset that Pandas interpreted as strings, all that remains is to clean up the remaining columns.\n",
    "\n",
    "<h4>Designation</h4>\n",
    "\n",
    "Designation contains what 'category' a wine is. This should be a categorial variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"designation\"] = dataset[\"designation\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "<h4>Points</h4>\n",
    "\n",
    "Points contains discrete values that represent review-points. Data-exploration showed that this contained no unexpected-values, and is stored as an integer. So this needs no extra work.\n",
    "\n",
    "<h4>Price</h4>\n",
    "\n",
    "Data exploration showed that price is treated as an integer, which makes sense. The histogram in data exploration showed that most measurements fall within the 0-100 range, with some in the 100-200 range. Notably, the x-axis on the histogram stretches all the way to around 400, with no measurements visible (probably because the frequency is so low). Let's see whether there's some hard to see measurements there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset[dataset[\"price\"] > 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look up these wines by hand just to be sure these are accurate.\n",
    "<a href=\"https://www.vivino.com/US/en/casa-ferreirinha-barca-velha-douro/w/75975?year=2008\">Wine 833</a> is 638.48 dollars at the time of writing.\n",
    "<a href=\"https://www.wine-searcher.com/find/casa+ferreirinha+barca+velha+douro+portugal/2004/europe\">Wine 1078</a> is 633 dollars at the time of writing.\n",
    "\n",
    "It makes sense that these wines have gotten more expensive since the dataset was created. We'll take this as enough evidence to take these prices as fact, and not change them.\n",
    "\n",
    "<h4>Province</h4>\n",
    "\n",
    "Province contains strings, and should be categorical. Let's change them to categories, and then have a look at the values.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset[\"province\"] = dataset[\"province\"].astype(\"category\")\n",
    "dataset[\"province\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are some probable mistakes in there. Most notably, different language versions of the samen regions, and 'wine regions' being used instead of provinces. Let's go by the most important ones:\n",
    "<ol>\n",
    "<li>Alentejano isn't a province, and not some other place in portugal as well. We will assume that this is an an English version of Alentejo, which is an actual province.</li>\n",
    "<li><a href=\"https://www.portevinho.be/tejo\">Tejo is another word for Ribatejo</a></li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Bairrada_DOC\">Bairrada is a part of the Beira Litoral province</a></li>\n",
    "<li>DÃ£o is displayed incorrectly by jupiter, but <a href=\"https://www.quintadoriodao.com/ned/out/dao-wijngaarden-portugal.html\">is a wine region in Beira Alta province</a></li>\n",
    "<li>PenÃ­nsula de SetÃºbal is apart of the Estremadura province</li>\n",
    "<li>Beira Interior is apart of beira alta</li>\n",
    "</ol>\n",
    "\n",
    "We could continue this for a while, but that would take many hours of labor. We'll assume that the errors in the smaller categories won't influence any potential models too much, and possibly fix all the problems in the future if needed.\n",
    "\n",
    "For now, we'll take all categories with less then 50 members, and and them to a new 'Other' class.\n",
    "\n",
    "<i><a href=\"https://althistory.fandom.com/wiki/Provinces_of_Portugal_(Twilight_of_a_New_Era)\">Source for provinces</a></i>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset[\"province\"] = dataset[\"province\"].replace({\n",
    "    \"Alentejano\": \"Alentejo\",\n",
    "    \"Tejo\": \"Ribatejo\",\n",
    "    \"Bairrada\": \"Beira Litoral\",  # FIXME(m-jeu): Check wether this is correct\n",
    "    \"Dï¿½o\": \"Beira Alta\",\n",
    "    \"Penï¿½nsula de Setï¿½bal\": \"Estremadura\",\n",
    "    \"Beira Interior\": \"Beira Alta\"\n",
    "})\n",
    "\n",
    "province_freqs = dataset[\"province\"].value_counts()\n",
    "to_replace = province_freqs[province_freqs < 50].index.values\n",
    "dataset[\"province\"] = dataset[\"province\"].replace(to_replace, \"Other\")\n",
    "\n",
    "dataset[\"province\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's good for now.\n",
    "\n",
    "<h4>Winery</h4>\n",
    "\n",
    "Winery should be a categorical variable, so let's turn it into that.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset[\"winery\"] = dataset[\"winery\"].astype(\"category\")\n",
    "dataset[\"winery\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no obvious problems with it.\n",
    "\n",
    "<h4>Fixed acidity</h4>\n",
    "\n",
    "From data exploration we know Fixed acidity is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>Volatile acidity</h4>\n",
    "\n",
    "From data exploration we know Volatile acidity is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>Residual sugar</h4>\n",
    "\n",
    "From data exploration we know Residual sugar is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>Chlorides</h4>\n",
    "\n",
    "From data exploration we know Chlorides is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>Free sulfur dioxide</h4>\n",
    "\n",
    "From data exploration we know Free sulfur dioxide is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>Total sulfur dioxide</h4>\n",
    "\n",
    "From data exploration we know Total sulfur dioxide is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>pH</h4>\n",
    "\n",
    "From data exploration we know pH is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change.\n",
    "\n",
    "<h4>Sulphates</h4>\n",
    "\n",
    "From data exploration we know Sulphates is a continuous variable, which is stored as Float64, without problematic outliers. There is not much to change."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dropping unneeded columns</h3>\n",
    "\n",
    "We drop columns description, taster_name and id, because they are not useful for later models, and hold no sway over our target variable, points. We also drop raw_citric acid, raw_density and raw_alcohol because they are no longer needed in data preparation.\n",
    "\n",
    "Even though it's not a feature or target, we won't drop title, because we still need it later to help draw conclusions from the data.\n",
    "\n",
    "Leaving us with a dataset containing clean target and feature variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['country', 'variety', 'description', 'taster_name', 'id', 'raw_citric acid', 'raw_density', 'raw_alcohol'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Post-Preperation evaluation</h3>\n",
    "\n",
    "Having made sure all the data is clean, and stored in the right variable type, we can evaluate any possible (linear) correlations once more:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlation_matrix(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A next step in our research process is finding an external dataset to complement our existing one with, to help construct our models later in the process. Our original dataset came without source, or explanation of it's origin, but while looking for other datasets to add, we ran into <a href=\"https://www.kaggle.com/zynicide/wine-reviews\">this dataset</a> and <a href=\"https://archive.ics.uci.edu/ml/datasets/wine+quality\">this dataset</a>.\n",
    "The second dataset seems to correspond to the chemical properties contained in our original dataset. The first dataset seems to correspond to everything else in the original.\n",
    "\n",
    "We will start referring to the first (new) dataset as the review dataset, the second one as the chemical dataset, and our original dataset as the HU dataset.\n",
    "\n",
    "The chemical dataset is subdivided into 2 files, for red and white wine respectively.\n",
    "Let's load them in and have a look:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chem_dataset_red = pd.read_csv(\"datasets/winequality-red.csv\", sep=\";\")\n",
    "chem_dataset_white = pd.read_csv(\"datasets/winequality-white.csv\", sep=\";\")\n",
    "review_dataset = pd.read_csv(\"datasets/winemag-data-130k-v2.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chem_dataset_red.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chem_dataset_white.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "review_dataset.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking through the chemical dataset, it turns out to be <i>anonymised</i>, which is odd, because there are no discernible features to match the review dataset and the chemical dataset on, to create the HU dataset.\n",
    "\n",
    "The sources for the 2 datasets are also wildly different. The wine dataset was scraped from a website called by a Kaggle user WineEnthusiast in 2017, the chemical dataset has as source:\n",
    "\n",
    "Paulo Cortez, University of Minho, GuimarÃ£es, Portugal, http://www3.dsi.uminho.pt/pcortez\n",
    "A. Cerdeira, F. Almeida, T. Matos and J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal @2009\n",
    "\n",
    "A paper by a Portuguese professor into data mining from 2009.\n",
    "\n",
    "\n",
    "Looking back at the earlier correlation matrix with the realisation that the HU dataset might have been created from two separate datasets shows that there are no strong (linear) correlations between attributes from the chemical dataset, and from the review dataset, the strongest one is between the 'alcohol' and 'points' attributes, with a Pearson correlation coefficient of 0.47, which is mediocre at best.\n",
    "\n",
    "Interestingly, this correlation appears to be very close to the correlation between alcohol and 'quality' in both chemical datasets. In those, quality is another metric for a score that someone gave the wine:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Alcohol/Quality correlation in red chemical dataset:\")\n",
    "np.corrcoef(chem_dataset_red['alcohol'], chem_dataset_red['quality'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Alcohol/Quality correlation in white chemical dataset:\")\n",
    "np.corrcoef(chem_dataset_white['alcohol'], chem_dataset_white['quality'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This could mean that the datasets were matched on a correlation between the two. In case this is based on a more complicated polynomial/logarithmic correlation, let's scatter the two in the HU dataset to examine them visually:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title(\"Points vs. Alcohol Percentage in HU dataset.\")\n",
    "ax.set_xlabel(\"Alcohol percentage\")\n",
    "ax.set_ylabel(\"Points\")\n",
    "ax.scatter(dataset[\"alcohol\"], dataset[\"points\"], s=15)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no obvious complex correlations, because the data appears to be way too scattered to draw any conclusions from. This also means that basing a merge of 2 datasets on this would be questionable at best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To verify our dataset-merge hypothesis even further, we also decided to manually look up records from both the left and right dataset manually, and check whether they match. The easiest way to do this seems to be to look up a wine review (allegedly from the review dataset), and it's corresponding alcohol percentage:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset[[\"title\", \"alcohol\"]].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quinta dos Avidagos 2011 Avidagos Red (Douro)\n",
    "According to the dataset: 9.7%\n",
    "According to <a href=\"https://www.wine-searcher.com/find/quinta+dos+avidagos+tinto+douro+portugal/2011/netherlands#t2\">the internet</a>: Between 13 and 14 %\n",
    "\n",
    "Casa Santa Vitï¿½ria 2013 Grande Reserva Tinto Red (Alentejano)\n",
    "According to the dataset: 9.6%.\n",
    "According to <a href=\"https://www.wine-searcher.com/find/casa+de+santa+vittoria+grand+rsrv+regional+alentejano+alentejo+portugal/2013#t2\">the internet.</a>: 14%.\n",
    "\n",
    "Luis Duarte 2013 Monte de Carrapatelo Colheita Seleccionada Tinto Red (Alentejano) (index 5):\n",
    "According to the dataset: 12.3%\n",
    "According to <a href=\"https://www.specialtycellars.com/wp-content/uploads/2020/08/Monte-De-Carrapatelo-Tinto-2013.pdf\">the internet:</a> 14.5%\n",
    "\n",
    "\n",
    "<h3>Conclusion</h3>\n",
    "\n",
    "This all leads us to conclude that the HU dataset has probably been merged from 2 different datasets in at best an undocumented way, and at worst an illogical way. Combining this with the fact that there is no discernible source for the dataset, has lead us to decide that in the spirit of the scientific process with regard to transparency, we can't draw conclusions from the HU dataset in good faith. We will rephrase the questions asked to fit the review- and chemicaldataset and use those instead in the next iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}