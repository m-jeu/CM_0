{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837436ab",
   "metadata": {},
   "source": [
    "<h1> Research iteration 2</h1>\n",
    "\n",
    "<i>Sjoerd Beetsma, Maarten de Jeu\n",
    "Class V2A - Group 5</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b320b4",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "Like mentioned in research iteration 1, we have divided our 3 research questions across 3 datasets. The first 2 research questions correspond to the <i>Chemical</i> datasets, and the last questions corresponds to the <i>review</i> dataset.\n",
    "\n",
    "Throughout this notebook, we'll attempt to stick to the iterative CRISP-DM workflow as much as possible, and divide our notebook into chapters corresponding to phases from this philosophy. This chapter division is only a rough indication of what can be found where though, because the highly iterative workflow requires us to look back/forward in the process quite frequently.\n",
    "\n",
    "Specifically, everything you're looking for can be found in this order:\n",
    "\n",
    "<ol>\n",
    "<li>Business understanding: Chemical dataset.</li>\n",
    "<li>Data understanding: Chemical dataset.</li>\n",
    "<li>Data preparation: Chemical dataset.</li>\n",
    "<li>Modelling: Research question 1.</li>\n",
    "<li>Modelling: Research question 2.</li>\n",
    "<li>Business understanding: Review dataset.</li>\n",
    "<li>Data understanding: Review dataset.</li>\n",
    "<li>Data preparation: Review dataset.</li>\n",
    "<li>Modelling: Research question 3.</li>\n",
    "</ol>\n",
    "\n",
    "TODO(m-jeu): Evaluation after every modelling phase? Or combine into 1 chapter perhaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2cbadc",
   "metadata": {},
   "source": [
    "<h2>Business Understanding: Chemical dataset.</h2>\n",
    "\n",
    "The context for our research questions is quite vague from the side of our client, but we'll be examining the following questions:\n",
    "\n",
    "<ol>\n",
    "<li>With what accuracy can we decide the quality of a red-wine according to its chemical properties.</li>\n",
    "<li>Can we predict a wine's color based on it's chemical properties?</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45ad40",
   "metadata": {},
   "source": [
    "<h2> Data Understanding: Chemical datasets.</h2>\n",
    "\n",
    "The dataset for the first research question is aquired from https://archive.ics.uci.edu/ml/datasets/wine+quality along a dataset about red-wine there is also a dataset about white-wine, the first will be used for our first research question and the datasets combined will be used for our second research question.\n",
    "\n",
    "The variables in the chemical datasets are:<br />\n",
    "\n",
    "1 - fixed acidity: Fixed acids found in wines are tartaric, malic, citric, and succinic.   <br />\n",
    "2 - volatile acidity: Steam distillable acids like acetic acid, lactic, formic, butyric, and propionic acids.  <br />\n",
    "3 - citric acid: One of the fixed acids found in a wine <br />\n",
    "4 - residual sugar: Residual sugar in a wine is the natural grape sugars leftover in a wine after the fermentation is finished. <br />\n",
    "5 - chlorides: Chlorides are a major conributor to the saltiness of a wine.<br />\n",
    "6 - free sulfur dioxide:The part of sulfur that remains free after the sulfur is binded <br />\n",
    "7 - total sulfur dioxide:Free sulfur dioxide plus binded sulfur dioxide. <br />\n",
    "8 - density: Density of a wine, usually increased by fermentable sugars. <br />\n",
    "9 - pH: pH to specify the acidity/basicity of a wine, most wines ranging with a pH from 3 to 4. <br />\n",
    "10 - sulphates: Use of sulpates slows down the growth of yeasts keeping and prevents the oxidation keeping the wine fresh for longer. <br />\n",
    "11 - alcohol: Alcohol percentage of a wine <br />\n",
    "12 - quality: A quality score between 0 and 10, based on sensory data <br />\n",
    "\n",
    "The source of the data also states that they don't know if all variables are relevant in deciding the quality score of a wine.\n",
    "\n",
    "#todo add source of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e40312",
   "metadata": {},
   "source": [
    "We import some libraries and the dataset to examine the data through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "# FIXME(m-jeu): Is this necessary. Ik stel voor dat we allebei individueel ons gebruik van dingen hieruit omschrijven naar sk.nogwat.nogwat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import cluster\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import py_lib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5337c6c",
   "metadata": {},
   "source": [
    "For research question one we will only need the red-wine dataset. The second research question requires a combined dataset with labels added for the color of the wine (white or red) the two datasets will be explored and cleaned at the same time but separately, performing similar operations on both. Merging the two datasets before the data-cleaning would result in records being designated outliers that shouldn't be, correlations not being detected in certain datasets that should be, etc. We'll merge the two datasets after cleaning them for use in the second research question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126710ed",
   "metadata": {},
   "source": [
    "Let's load in both red and white wine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red = pd.read_csv(\"datasets/winequality-red.csv\", sep=\";\")\n",
    "dataset_white = pd.read_csv(\"datasets/winequality-white.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193babcb",
   "metadata": {},
   "source": [
    "Let's take the head of one of the chemical property datasets to have a first look at the submissions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c946f",
   "metadata": {},
   "source": [
    "As described by the source each row seems to correspond with a individual wine. With eleven different feature columns describing the chemical properties of the wine and one target column, quality.\n",
    "\n",
    "To access columns easier in the future change white spaces in column names to underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.columns = dataset_red.columns.str.replace(' ','_')\n",
    "dataset_white.columns = dataset_white.columns.str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489550b8",
   "metadata": {},
   "source": [
    "To see howmuch data entries we have we will check the amount of rows in both the red and white datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_rows, red_columns = dataset_red.shape\n",
    "white_rows, white_columns = dataset_white.shape\n",
    "\n",
    "print(f'The red-wine quality dataset contains {red_rows} rows and {red_columns} columns')\n",
    "print(f'The white-wine quality dataset contains {white_rows} rows and {white_columns} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3d190",
   "metadata": {},
   "source": [
    "Lets change the column name white spaces to underscores to make life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de320c",
   "metadata": {},
   "source": [
    "<h3>Target and feature variables</h3>\n",
    "\n",
    "For research question 1, all the columns describing chemical properties will be considered as a feature variable and the column quality represents the target variable, the variable we want to predict.\n",
    "Lets safe them in a variables to access them easily from the final dataframe.\n",
    "\n",
    "The feature variables (at the start) for research question are the same ones as those for question 1, but will change after some investigation, and the target variable is currently not available as column (the eventual target variable will be whichever dataset the record is in now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067527b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_vars_q1 = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "target_var_q1 = 'quality'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1af25",
   "metadata": {},
   "source": [
    "<h3> Scales of measurements </h3>\n",
    "\n",
    "To choose a appropriate model for our research-questions and available data it's necessary to have a understanding of all the scales of measurements for all the relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792898fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomi, disc, ordi, cont = 'Nominal', 'Discrete', 'Ordinal','Continous'\n",
    "print('Scales of measurement chemical properties datasets')\n",
    "pd.DataFrame(index=dataset_red.columns, data=[cont, cont, cont, cont, cont, cont, cont, cont, cont, cont, cont, disc],\n",
    "             columns=['Scale of measurement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488508a6",
   "metadata": {},
   "source": [
    "As can be seen all the chemical properties have continous scale of measurement and the quality columns has a Discrete scale of measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435838e9",
   "metadata": {},
   "source": [
    "<h3>Central tendancies and dispersion measures</h3>\n",
    "\n",
    "Using describe we can see some important central tendancies and dispersion measures about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c251f48",
   "metadata": {},
   "source": [
    "From the describe we can tell that there are quite a few columns with a big difference between maximum and minimum values which indicate outliers. <b>TODO(m-jeu): Does this indicate outliers? If std is large aswell, it doesn't have to right?</b>\n",
    "\n",
    "The columns with big differences between max and min values:\n",
    "Residual_sugar, chlorides, free_sulfur_dioxide ,total_sulfur_dioxide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_white.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7bd21",
   "metadata": {},
   "source": [
    "Just like the red-wine dataset, the white-wine dataset has similair differences in maximum and minimum values.\n",
    "\n",
    "Lets take a more visual look at the distribution of all data through a histogram for each of the feature attributes\n",
    "Starting off with the red wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af007820",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.hist(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784dbe5",
   "metadata": {},
   "source": [
    "In the the red-wine dataset pH and possibly density have a gaussian distribution. Other variables seem to have a skewed distribution.<br/>\n",
    "Moving on to the white-wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b2053",
   "metadata": {},
   "source": [
    "Moving on to the white wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_white.hist(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3933533",
   "metadata": {},
   "source": [
    "In the the white-wine dataset only pH seems to have a gaussian distribution maybe density aswell after removing outliers. Other variables have a skewed, possibly lognormal, distribution.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9877ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=dataset_red, x='quality').set_title('red-wine quality counts')\n",
    "plt.show()\n",
    "sns.countplot(data=dataset_white, x='quality').set_title('white-wine quality counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aed239",
   "metadata": {},
   "source": [
    "From the count plots above we can tell the quality for red-wines range from 3 to 8 with 5 being the most common quality rating. For white-wines it ranges from 3 to 9 with 6 being the most common quality rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a90b10",
   "metadata": {},
   "source": [
    "<h3>Outliers</h3>\n",
    "\n",
    "To get a visual understanding of the outliers in the feature columns each feature gets a boxplotted with the Q1 target variable quality. Giving a small summary of the minimum, Q1, Q2 (median), Q3 and the maximum of each attribute plotted against quality scored to give a view of outliers at all quality levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d42488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplotter(dataset, y_axes, x_axis):\n",
    "    \"\"\"Function to boxplot 1 x_axis against a list of y_axis of a given dataset\"\"\"\n",
    "    for col in y_axes:\n",
    "        sns.boxplot(x=dataset[x_axis], y=dataset[col])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68a1e5",
   "metadata": {},
   "source": [
    "First boxplot all the feature variables on the y-axis against the Q! target variable on the x-axis for the red-wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a04dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotter(dataset=dataset_red, y_axes=feature_vars_q1, x_axis=target_var_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdda52",
   "metadata": {},
   "source": [
    "Now do the same for the white-wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7475571",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotter(dataset=dataset_white, y_axes=feature_vars_q1, x_axis=target_var_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a0b24",
   "metadata": {},
   "source": [
    "As can be seen from the boxplots all of our current variables contain outliers.\n",
    "\n",
    "All outliers in the above boxplots seem to be plausible and not from incorrect data, like some attributes in iteration 1 had.\n",
    "From the boxplot with alcohol on the y axis and quality on the x axis we can  see that a trend of a rising median alcohol percentage the higher the quality of the wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79895d99",
   "metadata": {},
   "source": [
    "<h3>Correlations</h3>\n",
    "\n",
    "For later models it's important to know what variables have a (linear) correlation between each other.\n",
    "To find linear correlations and their direction/strength we make use of Pearson's correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f47eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix_plotter(dataset, title=''):\n",
    "    \"\"\"Return a correlation matrix created using seaborn and matplotlib that for all columns in\n",
    "    a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset to construct correlation matrix for.\n",
    "        title: Title of the plot.\n",
    "\n",
    "    Returns:\n",
    "        correlation matrix.\"\"\"\n",
    "    corr = dataset.corr()\n",
    "    plt.figure(figsize=(10,7.5))\n",
    "    cmap = sns.diverging_palette(200, 0, as_cmap=True) # color palette as cmap\n",
    "    mask = np.logical_not(np.tril(np.ones_like(corr))) # triangle mask\n",
    "    sns.heatmap(corr, annot=True, mask=mask, cmap = cmap, vmin=-1, vmax=1).set_title(title) # correlation heatmap\n",
    "    plt.show()\n",
    "\n",
    "corr_matrix_plotter(dataset_red, 'Red wine correlation matrix.')\n",
    "corr_matrix_plotter(dataset_white, 'White wine correlation matrix.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122293e",
   "metadata": {},
   "source": [
    "In the correlation matrices graphed above you can see which attributes have a correlation to other attributes. Starting with Q1 our target variable 'quality', we can see quality has a few correlations with the strongest one being alcohol for both red and white wines and a few weaker ones like volatile acidity, sulphates and citric acid for red wines and density and chloride for white wines. Because quality is our Q1 target variable it's the independent attribute in the correlations.\n",
    "\n",
    "Quality is dropped for research question 2, so for that it's not relevant.\n",
    "\n",
    "Besides there are some corelations among chemical properties:\n",
    "Fixed acidity has strong correlation with pH, but it’s still an independent type. pH However is a dependent type; it depends on the former. Volatile acidity, residual sugar, sulphates, chlorides, and density are all independent data types. Total sulfur dioxide is dependent on free sulfur dioxide, but free sulfur dioxide is independent.\n",
    "\n",
    "Strong correlations along features might need to be removed during the data preparation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e1a65",
   "metadata": {},
   "source": [
    "<h3>Data Preparation: Chemical dataset.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c720bd",
   "metadata": {},
   "source": [
    "Lets start of the data preparation by checking the datatypes and clean or change them if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1b2b5",
   "metadata": {},
   "source": [
    "Red-wine quality datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489a509",
   "metadata": {},
   "source": [
    "White-wine quality datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7362924",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_white.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703501d",
   "metadata": {},
   "source": [
    "These all seem to be in order, so we can now move on to checking and removing or replacing any NA values in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11665efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(dataset_red).sum().sum() # checking for total NA values in red_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(dataset_white).sum().sum() # checking for total NA values in white_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a07fa",
   "metadata": {},
   "source": [
    "But luckily, all data seems to be complete, so no need to worry about that either.\n",
    "\n",
    "<h4>Removing outliers</h4>\n",
    "\n",
    "A remove outliers function is created but and used to remove extreme outliers.\n",
    "This is will make a regression algorithm peform better because the RMSE is sensitive to outliers.\n",
    "\n",
    "Removing all extreme the outliers leaving the mild ones in the dataset with a outer fence.\n",
    "3 * IQR below Q1 or 3 * IQR above Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataset, fence = 3):\n",
    "    \"\"\"\n",
    "    Function to remove any rows containing one or more outliers in a dataframe.\n",
    "    Default fence of 3 * IQR detecting extreme outliers.\n",
    "\n",
    "    args:\n",
    "        dataset: Pandas Dataframe or Series\n",
    "        fence: Number deciding the fence range to use to detect outliers.\n",
    "    \n",
    "    returns:\n",
    "        The passed dataset minus the rows containing outliers\n",
    "    \"\"\"\n",
    "    q1 = dataset.quantile(.25)\n",
    "    q3 = dataset.quantile(.75)\n",
    "    iqr = q3 - q1\n",
    "    return dataset[(dataset >= q1 - (fence * iqr)) & (dataset <= q3 + (fence * iqr))].dropna() # turn extreme outliers into NaN values and drop the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae470263",
   "metadata": {},
   "source": [
    "The red-wine dataset contained 1599 rows and the white-wine 4898 before removing the outliers lets remove the outliers and check how many are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe428d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red = remove_outliers(dataset_red)\n",
    "dataset_white = remove_outliers(dataset_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd11941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red.shape, dataset_white.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d6367",
   "metadata": {},
   "source": [
    "1435 of the 1599 rows are left in the red-wine data, 12% of the rows contained etreme outliers.<br/>\n",
    "And in the white-wine data 4690 of the 4898 rows are left, 4% of the rows contained extreme outliers.<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e7bf5",
   "metadata": {},
   "source": [
    "<h3>Duplicates</h3>\n",
    "\n",
    "Duplicate values would probably cause overfitting in the worst case, and imbalanced models in the best case. Let's check if either dataset contains any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c46d53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_white.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5ae17",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_red.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f0c13",
   "metadata": {},
   "source": [
    "Both contain duplicates that need to be removed, so let's get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde88cb7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_white.drop_duplicates(inplace=True)\n",
    "dataset_red.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ab308",
   "metadata": {},
   "source": [
    "The red and white wine chemical property datasets are cleaned but still needs normalizing. Because we don't need normalized data for our first research question and only for our second one we will only normalize the red and white combined dataset.\n",
    "\n",
    "Now both the red and white dataset are ready to be combined into one dataset with a extra column for being red or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5dcac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_red['is_red'] = 1\n",
    "dataset_white['is_red'] = 0\n",
    "dataset_red_white = pd.concat([dataset_red, dataset_white])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_red_white.sample(10,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9717c81",
   "metadata": {},
   "source": [
    "<h4>Normalizing data</h4>\n",
    "\n",
    "Machine learning algorithms using a euclidean distance function benefit from working with normalized data.<br/>\n",
    "We'll define a function that normalizes a pandas Series or Dataframe object. We won't use it for now, because it best to only do it when needed, but it's good to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s: pd.Series or pd.DataFrame) -> pd.Series or pd.DataFrame:\n",
    "    \"\"\"Normalize (standardize) a pandas series or dataframe object by subtracting the mean and dividing by the\n",
    "    standard deviation.\n",
    "\n",
    "    Args:\n",
    "        s: series object to normalize.\n",
    "\n",
    "    Returns:\n",
    "        normalized series object.\"\"\"\n",
    "    return (s - np.mean(s)) / np.std(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbeef0a",
   "metadata": {},
   "source": [
    "<h4>Data cleaned</h4>\n",
    "\n",
    "We now have the dataset_red, dataset_white, and dataset_red_white Dataframes, which are all cleaned. Nothing is normalized as of yet, but we have defined a procedure to do this with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17860fcb",
   "metadata": {},
   "source": [
    "<h3>Modeling: Chemical datasets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c56189",
   "metadata": {},
   "source": [
    "<h4>Test and train data</h4>\n",
    "\n",
    "The datasets will be splitted into a train and test dataset for the models to learn and test their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset_red[feature_vars_q1], dataset_red[target_var_q1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scores = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14b152",
   "metadata": {},
   "source": [
    "<h4>Baseline model</h4>\n",
    "\n",
    "We start out by creating a baseline model that always predicts the mean of the target variable in the training set, so that we can attempt to improve this score with other models. Let's check the RMSE of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = py_lib.DumbRegressor(y_train)\n",
    "base_line_predictions = baseline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e613af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(base_line_predictions, c='r')\n",
    "plt.scatter(x=np.arange(len(X_test)), y=y_test, s=4)\n",
    "plt.xlabel(\"Testset measurement\")  # FIXME(m-jeu): Better xlabel?\n",
    "plt.ylabel(\"Quality score\")\n",
    "plt.legend([\"Baseline prediction\", \"Actual value\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse = sk.metrics.mean_squared_error(y_test, base_line_predictions, squared=False)\n",
    "scores = scores.append({'algorithm':'baseline', 'RMSE': baseline_rmse},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9276b27",
   "metadata": {},
   "source": [
    "The baseline model scores a RMSE of .82 quality points, this score is what we want to improve upon with further models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba3727",
   "metadata": {},
   "source": [
    "<h4>Implementing a machine learning model</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c84b81",
   "metadata": {},
   "source": [
    "For our first machine learning model we will implementing the simpelest form of multiple feature regression: multiple linear regression.\n",
    "This model doesn't have any hyper-parameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c577d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple linear regression\n",
    "lin_regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ed5b9",
   "metadata": {},
   "source": [
    "Fit the model to the X train and y train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8383b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d99d3c",
   "metadata": {},
   "source": [
    "Get the mean_squared_error to see how good the prediction is vs the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# FIXME(m-jeu): Volgens mij is er vanaf hier iets mis gegaan in het kopieren, dus daar moeten we mss morgen nog ff naar kijken.\n",
    "# Ik heb y_test_red naar y_test veranderd, omdat ik dacht dat die 2 redundant van elkaar waren, maar mss was dat wel dom van me.\n",
    "# Ik pas vanaf hier tot research question 2 vanavond even niks meer aan.\n",
    "#################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b534d",
   "metadata": {},
   "source": [
    "Lets see how multiple linear regression compares to the baseline model. \n",
    "Get the RMSE to decide the peformance of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc516b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin_regr_model_score = sk.metrics.mean_squared_error(lin_regr.predict(X_test), y_test, squared=False)\n",
    "scores = scores.append({'algorithm':'multiple linear regression', 'RMSE': lin_regr_model_score},ignore_index=True)\n",
    "lin_regr_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca907e",
   "metadata": {},
   "source": [
    "The multiple linear regression scored a RMSE 0.61 compared to our baseline this is a 26% lower RMSE, meaning the baseline has already been improved surpassed by the first model. But it's not quite as low as we want it yet. Lets try to improve more with polynomial regression. Through polynomial regression we can see if the number of polynomials will result in a better peformance. A polynomial model with just 1 degree is the same as linear regeression.\n",
    "\n",
    "For our hyper-parameter, the degree of polynomials we will be trying 1 to 7 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial regression\n",
    "poly_r_degrees = np.arange(1,8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1c15d",
   "metadata": {},
   "source": [
    "Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d871f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_r_make_train_model = np.vectorize(lambda d: make_pipeline(PolynomialFeatures(d),linear_model.LinearRegression()).fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ab05c",
   "metadata": {},
   "source": [
    "Train a model for every degree earlier specified degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14505ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_r_models = poly_r_make_train_model(poly_r_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f427d76",
   "metadata": {},
   "source": [
    "Lets see how well the polynomial regression peformances with different degrees of polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbee467",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_r_models_scores = np.vectorize(lambda m: sk.metrics.mean_squared_error(m.predict(X_test), y_test, squared=False))(poly_r_models)\n",
    "for degree, score in enumerate(poly_r_models_scores):\n",
    "    print(f\"degree {degree+1}, score {score}\")\n",
    "scores\n",
    "scores = scores.append({'algorithm':'Polynomial regression', 'RMSE': np.min(poly_r_models_scores)},ignore_index=True)\n",
    "plt.plot(poly_r_degrees, poly_r_models_scores, 'bx-')\n",
    "# Add title and axis names\n",
    "plt.title('Polynomial degree VS RMSE score')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cbaa7",
   "metadata": {},
   "source": [
    "Polynomial regression peformed best with a polynomial degree of 1. This means polynomial regression won't have any benefit over linear regression.\n",
    "\n",
    "Lets also test ridge and lasso regression, these regression algorithms are less sensitive to correlations among feature variables.\n",
    "\n",
    "For Ridge regression we'll be trying out alphas #to-do.\n",
    "And for Lasso regression the alphas #to-do will be tested, since Lasso regression doesn't work with a alpha of 0.\n",
    "\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "ridge_r_alphas = np.arange(0,10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245e36e",
   "metadata": {},
   "source": [
    "Create different a vectorized function to create a Ridge model with a passed alpha setting, fitted to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_r_make_train_model = np.vectorize(lambda a: Ridge().set_params(alpha=a).fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8a222",
   "metadata": {},
   "source": [
    "Make the models with the above function and the ridge alphas from 0 to 10 with steps of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_r_models = ridge_r_make_train_model(ridge_r_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d862342",
   "metadata": {},
   "source": [
    "Test the models their peformance by predicting on the test dataset getting the RMSE from the prediction and the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_r_models_scores = np.vectorize(lambda m: sk.metrics.mean_squared_error(m.predict(X_test), y_test, squared=False))(ridge_r_models)\n",
    "plt.plot(ridge_r_alphas, ridge_r_models_scores, 'bx-')\n",
    "plt.show()\n",
    "print(min(ridge_r_models_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd217214",
   "metadata": {},
   "source": [
    "Ridge regression seems to peform better by a very small margin.\n",
    "\n",
    "Lastly lets try Lasso regression.\n",
    "\n",
    "Alphas that will are 0.001 to .1 with steps of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b703d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso regression\n",
    "lasso_r_alphas = np.arange(0.01, 1, 0.01) # Lasso doesn't start at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840bf3d",
   "metadata": {},
   "source": [
    "Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2acf58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso_r_make_train_model = np.vectorize(lambda a: Lasso().set_params(alpha=a).fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0440142",
   "metadata": {},
   "source": [
    "Train the model with the different alphas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ad048",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_r_models = lasso_r_make_train_model(lasso_r_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79b591",
   "metadata": {},
   "source": [
    "Get the RMSE to decide the peformance of the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_r_models_scores = np.vectorize(lambda m: sk.metrics.mean_squared_error(m.predict(X_test), y_test, squared=False))(lasso_r_models)\n",
    "plt.plot(lasso_r_alphas, lasso_r_models_scores, 'bx-'), plt.show()\n",
    "print(min(lasso_r_models_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28c099",
   "metadata": {},
   "source": [
    "Scatterplot of the two most correlating columns and the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.scatter_3d(x=X_test, y=X_test, z=y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae3f9e",
   "metadata": {},
   "source": [
    "<h3>Conclusion first model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67688992",
   "metadata": {},
   "source": [
    "Conclusion to be added. First model requires more tuning and adjusting before a final conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640c531",
   "metadata": {},
   "source": [
    "<h3>Modeling: research question 2.</h3>\n",
    "\n",
    "<h4>Test and train data</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb503982",
   "metadata": {},
   "source": [
    "For convenience, we'll add an overview of the 'combined' dataset. Then we'll split the dataset into a test and train portion. As a reminder: we'll be using the chemical properties as feature variables to try to determine whether a wine is red of white from these. The target variable is 'is_red', a nominal(boolean) variable with 2 possible values. We'll also standardize the feature variables, because we're using a model with euclidean distance measurements after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa4ccb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_red_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = normalize(dataset_red_white[feature_vars_q1]), dataset_red_white['is_red']\n",
    "X_train_red_white, X_test_red_white, y_train_red_white, y_test_red_white = train_test_split(X,y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942ec3f",
   "metadata": {},
   "source": [
    "<h4>Baseline model</h4>\n",
    "\n",
    "We'll start out by creating a simple model that always guesses the mode measurement of the target variable in the train dataset, and seeing what accuracy that gets, so that we have a score to attempt to improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8010f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "baseline = py_lib.DumbNominalClassifier(y_train_red_white)\n",
    "baseline_predictions = baseline.predict(X_test_red_white)\n",
    "baseline_predictions[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a44be",
   "metadata": {},
   "source": [
    "The majority of the test dataset consists of white wine, so that's what we'll blindly be guessing with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca257a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test_red_white, baseline_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30dafc",
   "metadata": {},
   "source": [
    "The baseline model is already able to get an accuracy of +/- 77.21% on the test dataset.\n",
    "\n",
    "We'll try to improve on this, starting with a simple model and moving towards more complex ones. We'll start out using KNearestNeighbors because of it's simplicity, but because of the over-representation of white wine, we suspect other models might be able to do better.\n",
    "\n",
    "Normally, one would build in business-knowledge to pick a few variables that might be relevant to start out with. In our case, we have no business expert, so we'll start with many attributes, and eliminate any unnecessary ones along the way.\n",
    "\n",
    "Hyper-parameter wise, we'll start by experimenting with different values for 'k'. Our classes are way bigger, but we'll call the max sensible value for k 100 for now, starting at 1, with any odd values being fair game because we only have 2 target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24509da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def knn_models(k_vals: np.ndarray,\n",
    "               X_train: pd.DataFrame,\n",
    "               y_train: pd.Series,\n",
    "               X_test: pd.DataFrame,\n",
    "               y_test: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Create, fit and score KNN-models with different k-values for one dataset parallel to each other\n",
    "    through numpy vectorized operations.\n",
    "\n",
    "    Args:\n",
    "        k_vals: values for k to train a model with.\n",
    "        X_train: feature variables training set.\n",
    "        y_train: target variables training set.\n",
    "        X_test: feature variables test set.\n",
    "        y_test: target variables test set.\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe that contains:\n",
    "            index column with every k-value.\n",
    "            'Model' column with the associated model.\n",
    "            'Score' column with the accuracy score that model got against the test-set.\"\"\"\n",
    "    models = np.vectorize(lambda k: neighbors.KNeighborsClassifier(n_neighbors = k))(k_vals)   # Initialize model objects.\n",
    "    models = np.vectorize(lambda m: m.fit(X_train, y_train))(models)                           # Train models.\n",
    "    scores = np.vectorize(lambda m: metrics.accuracy_score(y_test, m.predict(X_test)))(models) # Make predictions for test set and score.\n",
    "    return pd.DataFrame({\"Model\": models,                                                      # Create and return Dataframe.\n",
    "                         \"Score\": scores},\n",
    "                        index=k_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1, 101, 2)\n",
    "models = knn_models(ks, X_train_red_white, y_train_red_white, X_test_red_white, y_test_red_white)\n",
    "plt.plot(models.index, models[\"Score\"], 'bx-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36329866",
   "metadata": {},
   "source": [
    "It appears (k < 20 ∧ k > 0) is the most promising area, so let's have a look at the exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd567c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.loc[:20, \"Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a09a5",
   "metadata": {},
   "source": [
    "We're getting above 99% accuracy, with the peak around 99.60%. Out of all the choices with peak accuracy, it's good to pick a higher setting for k to avoid overfitting. k = 9 has this top score for instance, and would probably be a good choice for production.\n",
    "\n",
    "99.6% is an incredible score, which would mean a model is terribly useful. So incredible, that we should probably consider the fact that something might have gone wrong. We split our data into a training and a test test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b733a62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"Data shapes:\n",
    "   Train      Test\n",
    "X: {X_train_red_white.shape} {X_test_red_white.shape}\n",
    "y: {y_train_red_white.shape}    {y_test_red_white.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad5ae0",
   "metadata": {},
   "source": [
    "We removed duplicates that could end up in both the train and the test dataset from the dataset in data preparation.\n",
    "\n",
    "Perhaps certain chemical values really do have big discernible gaps between red and white wine. We know that most attributes have normal and lognormal distributions in either the red or the wine dataset individually, but one would expect there to be obvious differences between the distributions in either the red or wine dataset respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c81ada",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = dataset_white.loc[:, :\"alcohol\"].columns  # We don't need to look at quality or is_red.\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "fig.suptitle(\"Attribute value distributions in Red / White dataset\", fontsize=16)\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    white_view, red_view = dataset_white[col].to_numpy(), dataset_red[col].to_numpy()  # View of Numpy arrays contained in relevant cols.\n",
    "\n",
    "    # To enforce equal bins size between 2 histograms.\n",
    "    att_min = min(np.min(white_view), np.min(red_view))\n",
    "    att_max = max(np.max(white_view), np.max(red_view))\n",
    "    bins = np.linspace(att_min, att_max, 20)\n",
    "\n",
    "    #Actually making subplot\n",
    "    ax = fig.add_subplot(4, 3, i + 1)\n",
    "    ax.set_title(col)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.hist(dataset_white[col], alpha=0.5, bins=bins, color=\"yellow\")\n",
    "    ax.hist(dataset_red[col], alpha=0.5, bins=bins, color=\"r\")\n",
    "    ax.axvline(np.mean(white_view), color=\"darkorange\", linestyle=\"dashed\")\n",
    "    ax.axvline(np.mean(red_view), color=\"darkred\", linestyle=\"dashed\")\n",
    "    ax.legend([\"White mean\", \"Red mean\", \"White frequency\", \"Red Frequency\"], loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c79fd0",
   "metadata": {},
   "source": [
    "From this improved we can see that there are indeed some quite descriptive variables when it comes to predicting what color a wine is, which could explain the high accuracy.\n",
    "\n",
    "We can divide the attributes into 3 categories based on these distributions (based on visual inspection, and no formal math or logic of any kind):\n",
    "\n",
    "<ol>\n",
    "<li>Attributes that tell us a lot about the potential color of a wine.</li>\n",
    "<ul><li>Volatile Acidity</li><li>Chlorides</li><li>Total sulfur dioxide</li></ul>\n",
    "<li>Attributes that tell us a little, or might tell us a lot about the potential color of a wine</li>\n",
    "<ul><li>Fixed acidity</li><li>Residual sugar</li><li>Free sulfur dioxide</li><li>Density</li></ul>\n",
    "<li>Attributes that tell us close to nothing about the color of a wine</li>\n",
    "<ul><li>Citric acid</li><li>pH</li><li>Alcohol</li><ul>\n",
    "</ol>\n",
    "\n",
    "We'll synthesize some new Train/Test feature datasets from the old ones, based on the first category, to train some new models with to see if a stripped down model can perform just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547462b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat1 = [\"volatile_acidity\", \"chlorides\", \"total_sulfur_dioxide\"]\n",
    "X_train_red_white_2, X_test_red_white_2 = X_train_red_white[cat1], X_test_red_white[cat1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a1205",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = knn_models(ks, X_train_red_white_2, y_train_red_white, X_test_red_white_2, y_test_red_white)\n",
    "plt.plot(models.index, models[\"Score\"], 'bx-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1940ed7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models.loc[45:55, \"Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b21608",
   "metadata": {},
   "source": [
    "We can see that a KNN-model based on only 3 attributes (volatile_acidity, chlorides and total_sulfur_dioxide) can still predict what color a wine is with 99.20% accuracy when k = 47 (in the test set), which could also be interesting for an eventual model in production.\n",
    "\n",
    "Because we're dealing with so many Gaussian distributions, with distinctive differences between the target categories for some attributes, this problem could also be well suited for a Gaussian Naive Bayes Classifier.\n",
    "\n",
    "Even though the model we have right now already performs quite well, it could be interesting to try this out in an attempt to reach 100% accuracy. We'll try a Gaussian Naive Bayes classifier with both all the chemical properties, and the stripped down chemical properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345dcc76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_model_complete = naive_bayes.GaussianNB()\n",
    "nb_model_complete.fit(X_train_red_white, y_train_red_white)\n",
    "nb_model_complete.score(X_test_red_white, y_test_red_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfb4d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_model_stripped = naive_bayes.GaussianNB()\n",
    "nb_model_stripped.fit(X_train_red_white_2, y_train_red_white)\n",
    "nb_model_stripped.score(X_test_red_white_2, y_test_red_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca145c1",
   "metadata": {},
   "source": [
    "With 99.1% accuracy with all feature variables, and 97.8% with the stripped down feature variable list, the Gaussian Naive Bayes classifier performs worse then K Nearest Neighbors in both scenarios, which is quite unexpected. A possible real-world advantage of this could be the fact that this classifier could return the probabilities for a given record being in either class, which could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f3ed",
   "metadata": {},
   "source": [
    "<h3>Conclusion research question 2</h3>\n",
    "\n",
    "With the use of a fairly simple algorithm like K Nearest Neighbors, one can easily classify the color of wine based on certain chemical properties with an accuracy above 99%, even when just looking at volatile acidity, chlorides and total sulfur dioxide. Simplicity seems to be key in this example, because a more complex model like a Gaussian Naive Bayes classifier performs every so slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e94c79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3707a414",
   "metadata": {},
   "source": [
    "<h4>Implementing a machine learning model</h4>\n",
    "\n",
    "TODO(m-jeu): Actually move these headers to the proper place lmao.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c44b03",
   "metadata": {},
   "source": [
    "<h4>Hyper-parameters</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e9271",
   "metadata": {},
   "source": [
    "<h4>Validation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9e956",
   "metadata": {},
   "source": [
    "<h3>Research question 3</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505acbc",
   "metadata": {},
   "source": [
    "<h2> Business understanding </h2>\n",
    "\n",
    "For our third research question we have a dataset about wine reviews from different smelters with information about the origin of the wine, price and their review.\n",
    "<br/>\n",
    "With our research question being:\n",
    "<i>Can we distinguish between logical clusters of wineries? (Premium, budget, high-quality, etc...)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1289210",
   "metadata": {},
   "source": [
    "\n",
    "<h2> Data Understanding Wine-review dataset.</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc7e55",
   "metadata": {},
   "source": [
    "From our dataset's source ..., we have a list of the different attributes:  # FIXME(m-jeu): Source.\n",
    "#FIXME(m-jeu): Do we need to describe these?\n",
    "\n",
    "1 - country<br/>\n",
    "2 - description<br/>\n",
    "3 - designation<br/>\n",
    "4 - points<br/>\n",
    "5 - price<br/>\n",
    "6 - province <br/>\n",
    "7 - region_1<br/>\n",
    "8 - region_2<br/>\n",
    "9 - taster_name<br/>\n",
    "10 - taster_twitter_handle<br/>\n",
    "11 - title <br/>\n",
    "12 - variety<br/>\n",
    "13 - winery<br/>\n",
    "\n",
    "Let's load in the dataset and have a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429edd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reviews = pd.read_csv(\"datasets/winemag-data-130k-v2.csv\")\n",
    "dataset_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74178dd5",
   "metadata": {},
   "source": [
    "At first look it appears most of the (usable) variables are nominal, with points and price as the only numerical (discrete) values. We have some columns that initially seem fairly useless for the types of analysis that we will most probably be using for this project, like 'description', but we'll keep them just in case we end up doing anything like a sentiment-analysis type model. It also appears we have a redundant index columns called \"Unnamed: 0\".\n",
    "\n",
    "<h4>Target and feature variables</h4>\n",
    "\n",
    "We're not quite sure what feature variables we'll be using for the third question, but we know we'll be grouping by 'winery'. We'll start out by using 'price' and 'points' as further feature variables, but during the modelling stage we might end up using more.\n",
    "\n",
    "Considering we're looking for logical clusters (unsupervised learning), there are no target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e3915",
   "metadata": {},
   "source": [
    "<h4>Scales of measurement</h4>\n",
    "\n",
    "Like mentioned earlier, we're mostly dealing with categorical (ordinal, specifically) variables in this dataset. There are 2 numerical values. Points and price are both discrete values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c00b2d",
   "metadata": {},
   "source": [
    "<h4>Central tendencies and dispersion measures</h4>\n",
    "\n",
    "We can examine the spread of values of the numerical variables through histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dataset_reviews[[\"price\", \"points\"]].hist()  # _ = to prevent pointless table from showing on screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541bc6d",
   "metadata": {},
   "source": [
    "Points has an obvious Gaussian distribution. It does appear the price graph is made quite unreadable by some outliers. We'll have a proper look at those later, let's ignore them for now to have a better look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237865d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reviews[dataset_reviews[\"price\"] < 100][\"price\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c377d3",
   "metadata": {},
   "source": [
    "It appears that the price column in lognormally distributed.\n",
    "\n",
    "<h4>Outliers</h4>\n",
    "\n",
    "Because we won't be comparing against something, we'll be using a seperate way of creating boxplots to explore outliers for the numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edafd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=dataset_reviews[[\"points\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898f234",
   "metadata": {},
   "source": [
    "It appears the median' amount of points is around 88, and quite symmetric. There are a couple of outliers around 100, but nothing extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69310e39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=dataset_reviews[[\"price\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236d9dd",
   "metadata": {},
   "source": [
    "The boxplot for price is barely a boxplot because of all the outliers. Like we already noticed with the histogram, most measurements fall within the 0-100 range, but there are some extremely high outliers. For data exploration, we'll create a separate column with the outliers removed for price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f2ada",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reviews[\"price_no_outliers\"] = remove_outliers(dataset_reviews[\"price\"])  # FIXME(m-jeu): remove_outliers drops na\n",
    "dataset_reviews[\"points_no_outliers\"] = remove_outliers(dataset_reviews[\"points\"])# Which then get filled in again..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39163893",
   "metadata": {},
   "source": [
    "<h3>Correlations</h3>\n",
    "\n",
    "We'll use the pearson correlation coefficient to see if there's a linear correlation between the only 2 numerical columns in the dataset. Because of the extreme outliers in price, it might be sensible to also check this with the outliers removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0158d61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reviews[[\"points\", \"price\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f294fdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reviews[[\"points_no_outliers\", \"price_no_outliers\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa42c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It appears there is a weak linear correlation between price and points, when disregarding outliers. Because we're looking for cluster, this is not terribly relevant, but still noteworthy.\n",
    "\n",
    "<h2>Data preparation: Review dataset</h2>\n",
    "\n",
    "Having explored the data, we're ready to clean it up. We have already separated the outliers in a separate column in the dataset, because that couldn't wait until data preparation. First, let's get rid of the unnamed index column, because that's not useful in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cec5f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reviews.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "dataset_reviews.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302be9e",
   "metadata": {},
   "source": [
    "And let's change the categorical values from 'object', to 'string', to 'category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d8155",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reviews = dataset_reviews.convert_dtypes()\n",
    "\n",
    "categorical_vars = [\"country\", \"description\", \"designation\", \"province\", \"region_1\", \"region_2\", \"taster_name\", \"taster_twitter_handle\", \"title\", \"variety\", \"winery\"]\n",
    "\n",
    "dataset_reviews[categorical_vars] = dataset_reviews[categorical_vars].astype(\"category\")\n",
    "dataset_reviews.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009409e",
   "metadata": {},
   "source": [
    "<h2>Modelling: Review dataset</h2>\n",
    "\n",
    "Our goal is to find logical clusters for different types of wineries, using a clustering algorithm. First, we create a separate dataframe from the original dataframe grouped by winery, with the mean value of the point/price value of that winery's wine as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c76f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_winery = dataset_reviews.groupby(\"winery\")[[\"price_no_outliers\", \"points_no_outliers\"]].mean()\n",
    "dataset_winery.dropna(inplace=True)\n",
    "dataset_winery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3982e",
   "metadata": {},
   "source": [
    "For many clustering algorithms, it's useful to have the data normalized as well. Let's do that in seperate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f8bc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_winery[\"price_normalized\"] = normalize(dataset_winery[\"price_no_outliers\"])\n",
    "dataset_winery[\"points_normalized\"] = normalize(dataset_winery[\"points_no_outliers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748cb36",
   "metadata": {},
   "source": [
    "Let's have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49943533",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(dataset_winery[\"price_normalized\"], dataset_winery[\"points_normalized\"], s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfeb544",
   "metadata": {},
   "source": [
    "So far, it mostly just looks like a thick cloud. Hopefully, a clustering algorithm will be able to see through the fog and give us more insight.\n",
    "\n",
    "We'll start out by using kMeans on the (normalised) points and price because of it's simplicity, and move onto using more complex models and/or adding more data in case it doesn't give any useful results.\n",
    "Even though it's doubtful anything with k > 20 will be useful, we'll still try everything from k=2 to k=30, to see how the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bf040",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ks = np.arange(2, 31)  # Values for k to try out.\n",
    "\n",
    "models = np.vectorize(lambda k: cluster.KMeans(n_clusters=k, random_state=0))(ks)  # Create model objects.\n",
    "models = np.vectorize(lambda m: m.fit(dataset_winery[[\"price_normalized\", \"points_normalized\"]]))(models)  # Fit models.\n",
    "c1_models_scores = np.vectorize(lambda m: m.score(dataset_winery[[\"price_normalized\", \"points_normalized\"]]))(models)  # Score models\n",
    "plt.plot(ks, c1_models_scores, 'bx-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3920878",
   "metadata": {},
   "source": [
    "Considering the fact there's no clear 'elbow' in the elbow plot, and no clear clusters in the previously constructed graph, it doesn't look very promising. We can look at a visualization at k=6\n",
    "of the result to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ff3d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_model =models[4]  # Take it out of array for convenience.\n",
    "predictions = final_model.predict(dataset_winery[[\"price_normalized\", \"points_normalized\"]])\n",
    "plt.scatter(dataset_winery[\"price_no_outliers\"], dataset_winery[\"points_no_outliers\"], c=predictions, s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6ecc6",
   "metadata": {},
   "source": [
    "Like we expected, not very useful. It's doubtful another algorithm would be able to make sense out of that cloud, so the way to go is probably to attempt to use the curse of dimensionality to our advantage.\n",
    "There is one problem: there isn't much numerical data available to use in our models. All the categorical values in the dataset are nominal, so that would mean using a get_dummies() construction to turn them into useful data. Unfortunately, all the nominal variables have too many permutations to realistically use, so we'll have to get creative.\n",
    "\n",
    "In interesting metric could be the amount of wine reviews listed for a single winery. We'll create a new grouped by dataframe, with this column added, normalize it's columns, and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318320b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_reviews[\"winery\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b2869",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_winery_2 = dataset_reviews.groupby(\"winery\").agg({\"price_no_outliers\": np.mean,\n",
    "                                                          \"points_no_outliers\": np.mean,\n",
    "                                                          \"title\": lambda np_a: np_a.size})  # Title is a random column\n",
    "                                                                                             # Doesn't really matter what column\n",
    "                                                                                             # We pick here\n",
    "dataset_winery_2.rename(columns={\"title\": \"review_amount\"}, inplace=True)  # Rename columns to better name\n",
    "\n",
    "dataset_winery_2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c858234",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_winery_2[\"price_normalized\"] = normalize(dataset_winery_2[\"price_no_outliers\"])\n",
    "dataset_winery_2[\"points_normalized\"] = normalize(dataset_winery_2[\"points_no_outliers\"])\n",
    "dataset_winery_2[\"reviews_normalized\"] = normalize(dataset_winery_2[\"review_amount\"])\n",
    "dataset_winery_2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef32577",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(data_frame=dataset_winery_2,\n",
    "              x='price_normalized',\n",
    "              y='points_normalized',\n",
    "              z='reviews_normalized')\n",
    "\n",
    "fig.update_traces(marker={'size': 1, 'colorscale': 'Viridis', 'opacity': 0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b518755",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>Conclusion research question 3.</h3>\n",
    "\n",
    "Visually, we can see there's still no sensible clusters to be found. There is no more numerical data, and creating even more from what we have would be fairly pointless. Unfortunately, we have to conclude that there's no useful insight to be gained from clustering on our current data, as there just aren't many numbers to work with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
